[DecisionTree.hyper]

[DecisionTree.model]
ccp_alpha = 0.0
criterion = "gini"
min_impurity_decrease = 0.0
min_samples_leaf = 3
min_samples_split = 2
min_weight_fraction_leaf = 0.0
random_state = 239
splitter = "best"

[RandomForest.hyper]

[RandomForest.model]
bootstrap = true
ccp_alpha = 0.0
criterion = "gini"
max_features = "sqrt"
min_impurity_decrease = 0.0
min_samples_leaf = 1
min_samples_split = 4
min_weight_fraction_leaf = 0.0
n_estimators = 200
oob_score = false
random_state = 239
verbose = 0
warm_start = false

[XGBoost.hyper]

[XGBoost.model]
objective = "binary:logistic"
colsample_bytree = 1
enable_categorical = false
eval_metric = "logloss"
learning_rate = 0.1
max_depth = 5
missing = nan
n_estimators = 200
random_state = 239
subsample = 1

[AdaBoost.hyper]

[AdaBoost.model]
algorithm = "SAMME.R"
learning_rate = 1.0
n_estimators = 20
random_state = 239

[SVM.hyper]

[SVM.model]
C = 0.4
break_ties = false
cache_size = 200
coef0 = 0.0
decision_function_shape = "ovr"
degree = 3
gamma = "scale"
kernel = "rbf"
max_iter = -1
probability = true
random_state = 239
shrinking = true
tol = 0.001
verbose = false

[LightGBM.hyper]

[LightGBM.model]
boosting_type = "gbdt"
colsample_bytree = 1.0
importance_type = "split"
learning_rate = 0.1
max_depth = -1
min_child_samples = 20
min_child_weight = 0.001
min_split_gain = 0.0
n_estimators = 200
num_leaves = 30
objective = "multiclass"
random_state = 239
reg_alpha = 0.0
reg_lambda = 0.0
subsample = 1.0
subsample_for_bin = 200000
subsample_freq = 0
metric = "multi_logloss"
verbose = -1

[MLP.hyper]
epochs = 1000
lr = 0.001

[MLP.model]
